<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/wecan-covidhelp/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/wecan-covidhelp/" rel="alternate" type="text/html" /><updated>2021-05-03T22:27:27+02:00</updated><id>http://localhost:4000/wecan-covidhelp/feed.xml</id><title type="html">Debjyoti Bhattacharjee</title><subtitle>Snapshots of ideas and things that piqued my interest. The website has information primarily focused on my research in emerging technologies, such as analog-in-memory computing, quantum computing alongside computer architecture. </subtitle><author><name>Debjyoti Bhattacharjee</name><email>debjyoti [dot] bhattacharjee [at] imec [dot] be</email></author><entry><title type="html">Code Generation with TVM</title><link href="http://localhost:4000/wecan-covidhelp/machine/learning/tvm/2021/05/01/tvm-getting-started.html" rel="alternate" type="text/html" title="Code Generation with TVM" /><published>2021-05-01T01:32:56+02:00</published><updated>2021-05-01T01:32:56+02:00</updated><id>http://localhost:4000/wecan-covidhelp/machine/learning/tvm/2021/05/01/tvm-getting%20started</id><content type="html" xml:base="http://localhost:4000/wecan-covidhelp/machine/learning/tvm/2021/05/01/tvm-getting-started.html">&lt;p&gt;&lt;a href=&quot;https://tvm.apache.org/&quot;&gt;Apache TVM&lt;/a&gt; is yet another compiler framework (&lt;a href=&quot;https://github.com/ONNC/onnc&quot;&gt;ONNC&lt;/a&gt;, &lt;a href=&quot;https://mlir.llvm.org/&quot;&gt;MLIR&lt;/a&gt;, etc.) for machine learning workloads targeted at supporting variety of hardware backends. It supports “Just in Time compilation” for the supported backends. It represents the workloads using “Relay” program, which 
can be then lowered to “&lt;a href=&quot;https://tvm.apache.org/docs/api/python/ir.html#tvm.ir.Attrs.list_field_info&quot;&gt;TVM IR&lt;/a&gt;” and scheduled for execution.&lt;/p&gt;

&lt;p&gt;I started to look at the &lt;a href=&quot;https://tvm.apache.org/2020/07/15/how-to-bring-your-own-codegen-to-tvm&quot;&gt;tutorial&lt;/a&gt; for code generation using &lt;a href=&quot;https://github.com/oneapi-src/oneDNN&quot;&gt;DNNL&lt;/a&gt;. The tutorial provides a relatively comprehensive guide towards 
TVM with a custom code generator, which emits JSON for certain Relay operators marked for the specific code generation backend (in this case “dnnl”).&lt;/p&gt;

&lt;p&gt;By simply enabling the DNNL codegen in the config.cmake and compiling tvm, it is possible to generate the json from a model using the following piece of code.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mod = relay.transform.AnnotateTarget([&quot;dnnl&quot;])(mod)
mod = relay.transform.MergeCompilerRegions()(mod)
mod = relay.transform.PartitionGraph()(mod)
graph_json, lib, params = relay.build(mod, target='llvm')
print('Graph json:', graph_json)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In the first step, the operators supported by the backend are annotated, which are then merged. The relay graph is partitioned and using the “build” step, the code is generated. For the operators which are not supported by the code generator, these would be executed in TVM. The generated lib includes the host module and the execution modules for the part that cannot be offloaded to the custom backend accelerator. The host module will invoke the accelerator module in runtime when it executes a graph node that is annotated for the accelerator. Currently, TVM does not support ahead of time compilation, which would allow generation of executable at compile time itself, which made TVM unsuitable for my current work.&lt;/p&gt;

&lt;p&gt;In summary, the framework has a lot of features for optimizing machine learning workloads without a lot of effort. However, it might be a bit complex to get started off with the documentation fragmented over a variety of pages. A relatively good order to look at the tutorials in my opinion is listed below.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Good tutorial&lt;/strong&gt;: This tutorial provides the quickest way to understand the primitives of a relay program. &lt;a href=&quot;http://tvm.d2l.ai/chapter_getting_started/vector_add.html&quot;&gt;[Link]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TVM + Cuda&lt;/strong&gt; : A jupyter notebook that shows how to use CUDA as a backend. &lt;a href=&quot;https://github.com/andersy005/tvm-in-action/blob/master/tvm-tutorials/getting-started.ipynb&quot;&gt;[Link]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Beginner guide&lt;/strong&gt; : This is a good starting point to understand the TVM code base structure. &lt;a href=&quot;https://tvm.apache.org/docs/dev/codebase_walkthrough.html&quot;&gt;[Link]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Good code gen tutorial&lt;/strong&gt; : The tutorial provides a relatively comprehensive guide towards 
TVM with a custom code generator, which emits codes for certain Relay operators marked for the specific code generation backend.  &lt;a href=&quot;https://tvm.apache.org/2020/07/15/how-to-bring-your-own-codegen-to-tvm&quot;&gt;[Link]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Available relay passes&lt;/strong&gt;: A number of built-in passes are already provided to optimize the relay graph. However, the passes might emit errors if the input relay graph has custom operators. &lt;a href=&quot;https://tvm.apache.org/docs/api/python/relay/transform.html&quot;&gt;[Link]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Debjyoti Bhattacharjee</name><email>debjyoti [dot] bhattacharjee [at] imec [dot] be</email></author><category term="ai" /><category term="ml" /><category term="&quot;neural" /><category term="network&quot;" /><category term="tvm" /><category term="compiler" /><summary type="html">Apache TVM is yet another compiler framework (ONNC, MLIR, etc.) for machine learning workloads targeted at supporting variety of hardware backends. It supports “Just in Time compilation” for the supported backends. It represents the workloads using “Relay” program, which can be then lowered to “TVM IR” and scheduled for execution.</summary></entry><entry><title type="html">Welcome to my blog</title><link href="http://localhost:4000/wecan-covidhelp/eda/2020/03/04/first-post.html" rel="alternate" type="text/html" title="Welcome to my blog" /><published>2020-03-04T17:32:56+01:00</published><updated>2020-03-04T17:32:56+01:00</updated><id>http://localhost:4000/wecan-covidhelp/eda/2020/03/04/first-post</id><content type="html" xml:base="http://localhost:4000/wecan-covidhelp/eda/2020/03/04/first-post.html">&lt;p&gt;My keen interest in hardware technologies, coupled with my background in computer science, has driven me to pursue research in the direction of design automation algorithms and tools for architectures, for a variety of technologies. Recently, I am learning about the state-of-the-art in Deep Neural Networks~(NN) and understanding the NN strutures (&lt;a href=&quot;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&quot; target=&quot;_blank&quot;&gt;Alexnet&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot; target=&quot;_blank&quot;&gt;Resnet&lt;/a&gt;, etc).&lt;/p&gt;

&lt;p&gt;In parallel, I have started using &lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch&lt;/a&gt; for implementation of DNNs. For the purpose of designing efficient NN accelerators, the first step is to find a way to extract an exact description of the NN graph. I found out the possiblity of finding the execution traces of the defined NNs using &lt;a href=&quot;https://pytorch.org/docs/stable/jit.html#mixing-tracing-and-scripting&quot;&gt;Torchscript&lt;/a&gt; and then parsing them into a directed graph. However, this is almost an hack! A better alternative was to look at the Open Neural Network Exchange (&lt;a href=&quot;https://onnx.ai/&quot;&gt;ONNX&lt;/a&gt;) format for NNs. Each ONNX model is self contained and can be accessed as a &lt;a href=&quot;#proto&quot;&gt;ProtoBuf&lt;/a&gt; object. PyTorch and most other deep NN frameworks allows models to be directly written to ONNX format, which makes it a common representation to be used for the process of design automation of NN accelerators.&lt;/p&gt;

&lt;p&gt;In the upcoming blogs, expect more details of design automation process for NN accelerators to be unveiled.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&quot; target=&quot;_blank&quot;&gt;ImageNet Classification with Deep ConvolutionalNeural Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot; target=&quot;_blank&quot;&gt;Deep Residual Learning for Image Recognition&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developers.google.com/protocol-buffers/docs/overview&quot; target=&quot;_blank&quot;&gt;Introduction to protocol buffers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Debjyoti Bhattacharjee</name><email>debjyoti [dot] bhattacharjee [at] imec [dot] be</email></author><category term="introduction" /><category term="ai" /><category term="ml" /><category term="eda" /><category term="pytorch" /><summary type="html">My keen interest in hardware technologies, coupled with my background in computer science, has driven me to pursue research in the direction of design automation algorithms and tools for architectures, for a variety of technologies. Recently, I am learning about the state-of-the-art in Deep Neural Networks~(NN) and understanding the NN strutures (Alexnet, Resnet, etc).</summary></entry></feed>